{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0U1NkXx_7_b"
      },
      "source": [
        "# Assignment 9: Self-Supervised Learning - Kai Ponel & Hannan Mahadik\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqxwZshS6T01"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YK7zbxob70r8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "tfkl = tf.keras.layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN9z4geq6W1e"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVnMeUCd6ZdW",
        "outputId": "e3935c05-d0c5-41d0-f8a5-2c137a7c06e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 6s 0us/step\n"
          ]
        }
      ],
      "source": [
        "cifar10 = tf.keras.datasets.cifar10\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gRYmtgr16nHG"
      },
      "outputs": [],
      "source": [
        "X_train = train_images.astype('float32') / 255.0\n",
        "X_test = test_images.astype('float32') / 255.0\n",
        "\n",
        "y_train= train_labels.astype(np.int32).reshape((-1,))\n",
        "y_test = test_labels.astype(np.int32).reshape((-1,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E5HuvX0S6pOT"
      },
      "outputs": [],
      "source": [
        "batch_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_noise(batch, std):\n",
        "  shape = batch.shape[1:]\n",
        "  noise = np.random.normal(scale=std, size=shape)\n",
        "  return batch + noise\n",
        "\n",
        "dae_train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, X_train))\n",
        "  #.shuffle(1024)\n",
        "  .batch(batch_size)\n",
        "  .map(lambda X, y: (gaussian_noise(X, 0.1), y)))\n",
        "\n",
        "dae_test_dataset = (tf.data.Dataset.from_tensor_slices((X_test, X_test))\n",
        "  .shuffle(1024)\n",
        "  .batch(batch_size))"
      ],
      "metadata": {
        "id": "aL5NG7Xo63i-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X_noise_batch, X_batch in dae_train_dataset:\n",
        "  image = tf.cast(X_noise_batch[1] * 255, tf.int32)\n",
        "  plt.imshow(image, cmap = \"Greys_r\")\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "KdTo8Zxr61WE",
        "outputId": "31a17a72-28da-47c4-e2c7-b3191f3bc44f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cl_train_dataset = (tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "  .shuffle(1024)\n",
        "  .batch(batch_size))\n",
        "\n",
        "cl_test_dataset = (tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "  .shuffle(1024)\n",
        "  .batch(batch_size))"
      ],
      "metadata": {
        "id": "5t87Hoch67PZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Head"
      ],
      "metadata": {
        "id": "LvX3VFk96-Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationHead(tf.keras.models.Model):\n",
        "  def __init__(self, num_hidden_units, num_output_units):\n",
        "    super(ClassificationHead, self).__init__()\n",
        "\n",
        "    self.num_hidden_units = num_hidden_units\n",
        "    self.num_output_units = num_output_units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    \n",
        "    self.hidden_layer = tf.keras.layers.Dense(self.num_hidden_units, activation='ReLU',\n",
        "                                         input_shape = input_shape)\n",
        "    self.output_layer = tf.keras.layers.Dense(self.num_output_units, \n",
        "                                         input_shape = (input_shape[0], \n",
        "                                                        num_hidden_units))\n",
        "    \n",
        "  def call(self, input):\n",
        "    h = self.hidden_layer(input)\n",
        "    y_pred = self.output_layer(h)\n",
        "    \n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "ETFqNYTl6_-6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding_dim = 64\n",
        "num_hidden_units = 128\n",
        "num_output_units = 10"
      ],
      "metadata": {
        "id": "5Tqbr6Mc7Wxz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Encoder"
      ],
      "metadata": {
        "id": "J23HWHVD7Y8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "image_input = tf.keras.Input((32, 32, 3))\n",
        "flattened = tf.keras.layers.Flatten()(image_input)\n",
        "encoding1 = tf.keras.layers.Dense(256, activation='relu')(flattened)\n",
        "encoding2 = tf.keras.layers.Dense(encoding_dim, activation='relu')(encoding1)\n",
        "mlp_encoder = tf.keras.Model(image_input, encoding2)\n",
        "\n",
        "# Decoder\n",
        "code_input = tf.keras.Input((encoding_dim,))\n",
        "decoding1 = tf.keras.layers.Dense(256, activation='relu')(code_input)\n",
        "decoding2 = tf.keras.layers.Dense(32 * 32 * 3, activation='sigmoid')(decoding1)\n",
        "reshaped = tf.keras.layers.Reshape((-1, 32, 32, 3))(decoding2)\n",
        "mlp_decoder = tf.keras.Model(code_input, reshaped)\n",
        "\n",
        "# Autoencoder\n",
        "mlp_autoencoder = tf.keras.Model(image_input, mlp_decoder(mlp_encoder(image_input)))"
      ],
      "metadata": {
        "id": "c41c5cUJ7a2i"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "mlp_autoencoder.compile(optimizer = optimizer, loss = loss_fn)"
      ],
      "metadata": {
        "id": "QmYCkbdk7kDF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_autoencoder.fit(dae_train_dataset, epochs = 5, 1\n",
        "                    validation_data = dae_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zmq7F41g7p3r",
        "outputId": "3cf63c63-dcfe-4b8c-a07c-c143f969bd43"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "196/196 [==============================] - 32s 152ms/step - loss: 0.6895 - val_loss: 0.6903\n",
            "Epoch 2/5\n",
            "196/196 [==============================] - 30s 152ms/step - loss: 0.6892 - val_loss: 0.6897\n",
            "Epoch 3/5\n",
            "196/196 [==============================] - 30s 153ms/step - loss: 0.6892 - val_loss: 0.6896\n",
            "Epoch 4/5\n",
            "196/196 [==============================] - 30s 153ms/step - loss: 0.6892 - val_loss: 0.6895\n",
            "Epoch 5/5\n",
            "196/196 [==============================] - 30s 153ms/step - loss: 0.6891 - val_loss: 0.6894\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9fd032ebe0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Encoder"
      ],
      "metadata": {
        "id": "8EAg4uFN7zvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "image_input = tf.keras.Input((32, 32, 3))                                                    # 32x32x3\n",
        "conv1 = tf.keras.layers.Conv2D(32, 3, padding='same', activation = 'relu')(image_input)      # 32x32x32\n",
        "pool1 = tf.keras.layers.MaxPool2D()(conv1)                                                   # 16x16x32\n",
        "conv2 = tf.keras.layers.Conv2D(64, 3, padding='same', activation = 'relu')(pool1)            # 16x16x64\n",
        "pool2 = tf.keras.layers.MaxPool2D()(conv2)                                                   # 8x8x64\n",
        "conv3 = tf.keras.layers.Conv2D(128, 3, padding='same', activation = 'relu')(pool2)           # 8x8x128\n",
        "pool3 = tf.keras.layers.MaxPool2D()(conv3)                                                   # 4x4x128\n",
        "flattened = tf.keras.layers.Flatten()(pool3)\n",
        "encoding = tf.keras.layers.Dense(encoding_dim, activation = 'relu')(flattened)\n",
        "cnn_encoder = tf.keras.Model(image_input, encoding)\n",
        "\n",
        "# Decoder\n",
        "code_input = tf.keras.Input((encoding_dim,))\n",
        "dense = tf.keras.layers.Dense(4 * 4 * 128, activation = 'relu')(code_input)\n",
        "reshaped = tf.keras.layers.Reshape((4, 4, 128))(dense)                                       # 4x4x128\n",
        "deconv1 = tf.keras.layers.Conv2DTranspose(64, 5, activation='relu')(reshaped)                # 8x8x64\n",
        "deconv2 = tf.keras.layers.Conv2DTranspose(32, 9, activation='relu')(deconv1)                 # 16x16x32\n",
        "deconv3 = tf.keras.layers.Conv2DTranspose(3, 17, activation='sigmoid')(deconv2)              # 32x32x3\n",
        "cnn_decoder = tf.keras.Model(code_input, deconv3)\n",
        "\n",
        "# Autoencoder\n",
        "cnn_autoencoder = tf.keras.Model(image_input, cnn_decoder(cnn_encoder(image_input)))"
      ],
      "metadata": {
        "id": "sEdG8ARB71SM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "cnn_autoencoder.compile(optimizer = optimizer, loss = loss_fn)"
      ],
      "metadata": {
        "id": "HX2PQ_x772lC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_autoencoder.fit(dae_train_dataset, epochs = 5, \n",
        "                    validation_data = dae_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GT0D1VJ74rI",
        "outputId": "de052016-faaf-48e0-b170-c951da4b2802"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "196/196 [==============================] - 12s 26ms/step - loss: 0.6465 - val_loss: 0.6377\n",
            "Epoch 2/5\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.6094 - val_loss: 0.6339\n",
            "Epoch 3/5\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.5993 - val_loss: 0.6101\n",
            "Epoch 4/5\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.5916 - val_loss: 0.6191\n",
            "Epoch 5/5\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.5888 - val_loss: 0.6322\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9f74407b50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Denoising CNN Autoencoder"
      ],
      "metadata": {
        "id": "jOSrdiL68J8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "image_input = tf.keras.Input((32, 32, 3))                                                    # 32x32x3\n",
        "conv1 = tf.keras.layers.Conv2D(32, 3, padding='same', activation = 'relu')(image_input)      # 32x32x32\n",
        "pool1 = tf.keras.layers.MaxPool2D()(conv1)                                                   # 16x16x32\n",
        "conv2 = tf.keras.layers.Conv2D(64, 3, padding='same', activation = 'relu')(pool1)            # 16x16x64\n",
        "pool2 = tf.keras.layers.MaxPool2D()(conv2)                                                   # 8x8x64\n",
        "conv3 = tf.keras.layers.Conv2D(128, 3, padding='same', activation = 'relu')(pool2)           # 8x8x128\n",
        "pool3 = tf.keras.layers.MaxPool2D()(conv3)                                                   # 4x4x128\n",
        "flattened = tf.keras.layers.Flatten()(pool3)\n",
        "encoding = tf.keras.layers.Dense(encoding_dim, activation = 'relu')(flattened)\n",
        "denoising_cnn_encoder = tf.keras.Model(image_input, encoding)\n",
        "\n",
        "# Decoder\n",
        "code_input = tf.keras.Input((encoding_dim,))\n",
        "dense = tf.keras.layers.Dense(4 * 4 * 128, activation = 'relu')(code_input)\n",
        "reshaped = tf.keras.layers.Reshape((4, 4, 128))(dense)                                       # 4x4x128\n",
        "deconv1 = tf.keras.layers.Conv2DTranspose(64, 5, activation='relu')(reshaped)                # 8x8x64\n",
        "deconv2 = tf.keras.layers.Conv2DTranspose(32, 9, activation='relu')(deconv1)                 # 16x16x32\n",
        "deconv3 = tf.keras.layers.Conv2DTranspose(3, 17, activation='sigmoid')(deconv2)              # 32x32x3\n",
        "denoising_cnn_decoder = tf.keras.Model(code_input, deconv3)\n",
        "\n",
        "# Autoencoder\n",
        "denoising_cnn_autoencoder = tf.keras.Model(\n",
        "    image_input, denoising_cnn_decoder(denoising_cnn_encoder(image_input)))"
      ],
      "metadata": {
        "id": "pmuIgbZj8ObQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "denoising_cnn_autoencoder.compile(optimizer = optimizer, loss = loss_fn)"
      ],
      "metadata": {
        "id": "wfykxrvQ8ifP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "denoising_cnn_autoencoder.fit(dae_train_dataset, epochs = 5,\n",
        "                              validation_data = dae_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyY8ysvc8npO",
        "outputId": "a7136c05-bbf7-4aae-d9fd-926e1c4949c0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "196/196 [==============================] - 6s 25ms/step - loss: 0.6449 - val_loss: 0.6530\n",
            "Epoch 2/5\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.6081 - val_loss: 0.6173\n",
            "Epoch 3/5\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.5984 - val_loss: 0.6270\n",
            "Epoch 4/5\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.5934 - val_loss: 0.6135\n",
            "Epoch 5/5\n",
            "196/196 [==============================] - 5s 24ms/step - loss: 0.5905 - val_loss: 0.6342\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9f74407eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Classification"
      ],
      "metadata": {
        "id": "6v-zV_P38qGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_input = tf.keras.Input((32, 32, 3))\n",
        "flatten = tf.keras.layers.Flatten()(image_input)\n",
        "layer1 = tf.keras.layers.Dense(256, activation='relu')(flatten)\n",
        "layer2 = tf.keras.layers.Dense(encoding_dim, activation='relu')(layer1)\n",
        "layer3 = tf.keras.layers.Dense(256, activation='relu')(layer2)\n",
        "output = tf.keras.layers.Dense(10)(layer3)\n",
        "\n",
        "simp_mlp_class_model = tf.keras.Model(image_input, output)\n",
        "\n",
        "loss_function = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
        "simp_mlp_class_model.compile(loss = loss_function, optimizer = optimizer, metrics = metrics)"
      ],
      "metadata": {
        "id": "0nuczSKy8tek"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simp_mlp_class_model.fit(cl_train_dataset, epochs = 5, validation_data = cl_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYG3zjrV80qt",
        "outputId": "2588ea62-f3f5-4af9-fd39-0454853bd3d7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "196/196 [==============================] - 2s 6ms/step - loss: 1.9040 - categorical_accuracy: 0.1123 - val_loss: 1.7261 - val_categorical_accuracy: 0.1136\n",
            "Epoch 2/5\n",
            "196/196 [==============================] - 1s 7ms/step - loss: 1.6938 - categorical_accuracy: 0.1026 - val_loss: 1.6230 - val_categorical_accuracy: 0.1236\n",
            "Epoch 3/5\n",
            "196/196 [==============================] - 1s 7ms/step - loss: 1.6133 - categorical_accuracy: 0.1004 - val_loss: 1.5851 - val_categorical_accuracy: 0.1161\n",
            "Epoch 4/5\n",
            "196/196 [==============================] - 1s 7ms/step - loss: 1.5486 - categorical_accuracy: 0.0999 - val_loss: 1.5374 - val_categorical_accuracy: 0.1041\n",
            "Epoch 5/5\n",
            "196/196 [==============================] - 1s 7ms/step - loss: 1.5088 - categorical_accuracy: 0.0994 - val_loss: 1.5038 - val_categorical_accuracy: 0.0923\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9f5e75c940>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder with classification (non trainable encoder):\n"
      ],
      "metadata": {
        "id": "e4rBJCwfbVnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP "
      ],
      "metadata": {
        "id": "L50cxC1xbbGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_input = tf.keras.Input((32, 32, 3))\n",
        "mlp_encoder.trainable = False\n",
        "classifier1 = ClassificationHead(256, 10)\n",
        "mlp_classifier = tf.keras.Model(image_input, classifier1(mlp_encoder(image_input)))\n",
        "\n",
        "loss_function = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
        "mlp_classifier.compile(loss = loss_function, optimizer = optimizer, metrics = metrics)"
      ],
      "metadata": {
        "id": "KXg-J7-QbhZm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_classifier.fit(cl_train_dataset, epochs = 5, validation_data = cl_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AyoUpm7cs3K",
        "outputId": "5e3be3a3-9667-4f45-b705-3ae35ba4a748"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "196/196 [==============================] - 2s 7ms/step - loss: 2.2766 - categorical_accuracy: 0.3554 - val_loss: 2.2420 - val_categorical_accuracy: 0.4468\n",
            "Epoch 2/5\n",
            "196/196 [==============================] - 1s 6ms/step - loss: 2.2150 - categorical_accuracy: 0.2855 - val_loss: 2.1988 - val_categorical_accuracy: 0.3301\n",
            "Epoch 3/5\n",
            "196/196 [==============================] - 1s 7ms/step - loss: 2.1936 - categorical_accuracy: 0.2340 - val_loss: 2.1921 - val_categorical_accuracy: 0.2791\n",
            "Epoch 4/5\n",
            "196/196 [==============================] - 1s 7ms/step - loss: 2.1895 - categorical_accuracy: 0.2214 - val_loss: 2.1911 - val_categorical_accuracy: 0.2843\n",
            "Epoch 5/5\n",
            "196/196 [==============================] - 1s 7ms/step - loss: 2.1876 - categorical_accuracy: 0.2129 - val_loss: 2.1883 - val_categorical_accuracy: 0.2330\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9f5e70cac0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Autoencoder"
      ],
      "metadata": {
        "id": "dZ9ADzE3dFIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_input = tf.keras.Input((32, 32, 3))\n",
        "cnn_encoder.trainable = False\n",
        "classifier2 = ClassificationHead(256, 10)\n",
        "cnn_classifier = tf.keras.Model(image_input, classifier2(cnn_encoder(image_input)))\n",
        "\n",
        "loss_function = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
        "cnn_classifier.compile(loss = loss_function, optimizer = optimizer, metrics = metrics)"
      ],
      "metadata": {
        "id": "eXXYjwNZdLA6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_classifier.fit(cl_train_dataset, epochs = 5, validation_data = cl_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q7l4uAKdcmy",
        "outputId": "70547f44-ba33-4d09-e3a8-03b6276be0f0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 1.9421 - categorical_accuracy: 0.1083 - val_loss: 1.8066 - val_categorical_accuracy: 0.0667\n",
            "Epoch 2/5\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.7710 - categorical_accuracy: 0.0902 - val_loss: 1.7318 - val_categorical_accuracy: 0.0739\n",
            "Epoch 3/5\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.7123 - categorical_accuracy: 0.0891 - val_loss: 1.6896 - val_categorical_accuracy: 0.0745\n",
            "Epoch 4/5\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.6733 - categorical_accuracy: 0.0883 - val_loss: 1.6659 - val_categorical_accuracy: 0.0631\n",
            "Epoch 5/5\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.6442 - categorical_accuracy: 0.0887 - val_loss: 1.6250 - val_categorical_accuracy: 0.0723\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9f5e3d6a60>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN denoise Autoencoder"
      ],
      "metadata": {
        "id": "77xIIr_BdwJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_input = tf.keras.Input((32, 32, 3))\n",
        "denoising_cnn_encoder.trainable = False\n",
        "classifier3 = ClassificationHead(256, 10)\n",
        "cnn_den_classifier = tf.keras.Model(image_input, classifier3(denoising_cnn_encoder(image_input)))\n",
        "\n",
        "loss_function = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
        "cnn_den_classifier.compile(loss = loss_function, optimizer = optimizer, metrics = metrics)"
      ],
      "metadata": {
        "id": "UQUd95XTdvz7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_den_classifier.fit(cl_train_dataset, epochs = 5, validation_data = cl_test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvAV1Iq9eYGx",
        "outputId": "c24b2d46-0bc4-4982-a65d-e77866d48008"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "196/196 [==============================] - 2s 10ms/step - loss: 1.9175 - categorical_accuracy: 0.1008 - val_loss: 1.8052 - val_categorical_accuracy: 0.0937\n",
            "Epoch 2/5\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.7782 - categorical_accuracy: 0.0929 - val_loss: 1.7395 - val_categorical_accuracy: 0.0822\n",
            "Epoch 3/5\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.7206 - categorical_accuracy: 0.0916 - val_loss: 1.6914 - val_categorical_accuracy: 0.1016\n",
            "Epoch 4/5\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.6781 - categorical_accuracy: 0.0925 - val_loss: 1.6659 - val_categorical_accuracy: 0.0959\n",
            "Epoch 5/5\n",
            "196/196 [==============================] - 2s 9ms/step - loss: 1.6459 - categorical_accuracy: 0.0894 - val_loss: 1.6514 - val_categorical_accuracy: 0.1003\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9f5e3dbeb0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JqxwZshS6T01",
        "pN9z4geq6W1e",
        "LvX3VFk96-Fr",
        "J23HWHVD7Y8r",
        "8EAg4uFN7zvC",
        "jOSrdiL68J8H",
        "6v-zV_P38qGf",
        "e4rBJCwfbVnf",
        "L50cxC1xbbGY",
        "dZ9ADzE3dFIw",
        "77xIIr_BdwJ_"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}