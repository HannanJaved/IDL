{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ca5liZ3s9fuK",
        "WG03MGMq9lhl",
        "lCtSLLRZOhoQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1: Let the Tensors Flow - Hannan Mahadik"
      ],
      "metadata": {
        "id": "o-MBDiQUwM5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Imports"
      ],
      "metadata": {
        "id": "jxjEY1Cn9BC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive; \n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/ColabNotebooks/Intro to Deep Learning\")\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "\n",
        "from datasets import MNISTDataset"
      ],
      "metadata": {
        "id": "nTeXTVhsBXcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e51e0a3-612d-4aaa-e483-d4290ad4d8a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load MNIST Dataset "
      ],
      "metadata": {
        "id": "utU3OUV69Zjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# we can look at any of the images and the corresponding labels\n",
        "# say, image no. 155\n",
        "plt.imshow(train_images[155], cmap=\"Greys_r\")\n",
        "plt.show()\n",
        "print(train_labels[155])\n",
        "\n",
        "# images are \"flattened\" into vectors\n",
        "data = MNISTDataset(train_images.reshape([-1, 28*28]), train_labels, \n",
        "                    test_images.reshape([-1, 28*28]), test_labels,\n",
        "                    batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "KeGoRMVeCSCo",
        "outputId": "ea719cc9-2828-4847-db91-c1c4ad9abba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization - Flattening"
      ],
      "metadata": {
        "id": "9-L6eVXAdwyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here is an attempt at illustrating what flattening looks like\n",
        "reshaped = train_images[155].reshape((1, 28*28))\n",
        "plt.figure(figsize=(15, 0.1))\n",
        "plt.pcolormesh(reshaped, cmap=\"Greys_r\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "04bwLKFdds-i",
        "outputId": "eaa3c4be-cc4e-4233-cd4f-065776f5456f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x7.2 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert Images to NP Arrays"
      ],
      "metadata": {
        "id": "1V3poJ5gd0UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images.astype(np.float32) / 255\n",
        "test_images = test_images.astype(np.float32) / 255"
      ],
      "metadata": {
        "id": "uFUGUouTdvcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Initialization\n"
      ],
      "metadata": {
        "id": "Ca5liZ3s9fuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the linear model\n",
        "# weight dimensions are completely determined by input/output dimensions\n",
        "W = tf.Variable(np.zeros([28*28, 10]).astype(np.float32))\n",
        "b = tf.Variable(np.zeros(10, dtype=np.float32))\n",
        "\n",
        "def model(inputs):\n",
        "    return tf.matmul(inputs, W) + b\n",
        "\n",
        "# NOTE\n",
        "# the ONLY thing that you should have to change for a working MLP is to\n",
        "# - add more variables\n",
        "# - adapt the model function (don't forget activation functions)\n",
        "# - add the new variables in the gradient call/update below\n",
        "\n",
        "# parameters for the training process\n",
        "# this already offers potential for experimentation\n",
        "# - how many steps do we actually need to reach acceptable performance?\n",
        "# - what if we train for some absurd number of steps?\n",
        "# - what happens if we increase/decrease the learning rate?\n",
        "# - do learning rate and number of steps interact?\n",
        "# - ...\n",
        "train_steps = 2000\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "2DQxWKlFCSyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training (Original Model)"
      ],
      "metadata": {
        "id": "WG03MGMq9lhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "for step in range(train_steps+1):\n",
        "    image_batch, label_batch = data.next_batch()\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(image_batch)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logits, labels=label_batch))\n",
        "        \n",
        "    grads = tape.gradient(xent, [W, b])\n",
        "    W.assign_sub(learning_rate * grads[0])\n",
        "    b.assign_sub(learning_rate * grads[1])\n",
        "    \n",
        "    # every so often we print loss/accuracy\n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, label_batch),\n",
        "                             tf.float32))\n",
        "        print(\"Step {}. Batch loss: {} Batch accuracy: {}\".format(step+1, xent, acc))\n"
      ],
      "metadata": {
        "id": "De63j8C1CUta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Deep Model"
      ],
      "metadata": {
        "id": "IcRiy5jCHQQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Module - A named container for tf.Variables\n",
        "class mlp(tf.Module):\n",
        "  \n",
        "  def __init__(self, units_per_layer, activation_function, name=None):\n",
        "\n",
        "    # initialization\n",
        "    self.activation_function = activation_function\n",
        "    self.layers = len(units_per_layer) \n",
        "    self.params = {}\n",
        "\n",
        "    # weight and bias initialization for the hidden layers\n",
        "    for layer in range(1, self.layers):\n",
        "\n",
        "      # Input of hidden layer 2 = output of hidden layer 1\n",
        "      input_size = units_per_layer[layer - 1] \n",
        "      output_size = units_per_layer[layer] \n",
        "\n",
        "      # Weights cannot be zero => Initialized with values close to zero\n",
        "      weights = \"w{}\".format(layer)\n",
        "      self.params[weights] = tf.Variable(\n",
        "          np.random.randn(input_size, output_size).astype(np.float32) * 0.01, \n",
        "          name = weights\n",
        "      )\n",
        "\n",
        "      # Bias initialized as zero (same as 'shallow' model) \n",
        "      bias = \"b{}\".format(layer)\n",
        "      self.params[bias] = tf.Variable(\n",
        "          np.zeros(output_size, dtype = np.float32), \n",
        "          name = bias\n",
        "      )\n",
        "\n",
        "\n",
        "  def __call__(self, x):\n",
        "\n",
        "    logits = self.logits(x)\n",
        "\n",
        "    # Logits are 'pre-softmax values' => apply softmax activation function\n",
        "    activation_function = tf.nn.softmax(logits, axis = 1)\n",
        "\n",
        "    return activation_function\n",
        "\n",
        "\n",
        "  def logits(self, x):\n",
        "    activation = x\n",
        "\n",
        "    # Assign each of the hidden layers with an input and activation function\n",
        "    for layer in range(1, self.layers-1):\n",
        "      weights, bias = \"w{}\".format(layer), \"b{}\".format(layer)\n",
        "      z = activation @ self.params[weights] + self.params[bias]\n",
        "      activation = self.activation_function(z)\n",
        "\n",
        "    # Output layer \n",
        "    weights, bias = \"w{}\".format(self.layers-1), \"b{}\".format(self.layers-1)\n",
        "    logits = activation @ self.params[weights] + self.params[bias]\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "FDURNKJsCs_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Specification"
      ],
      "metadata": {
        "id": "JoEhWKlaBP7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for easy customization of number of units, layers, and activation function\n",
        "def results(x):\n",
        "\n",
        "  no_of_units = x['units']\n",
        "  no_of_layers = x['layers']\n",
        "  \n",
        "  model = [28*28] + [no_of_units for _ in range(no_of_layers)] + [10]\n",
        "\n",
        "  result = train(mlp(model, x['activation_function']), data, \n",
        "                 training_steps = x['train_steps'], \n",
        "                 learning_rate = x['learning_rate'])"
      ],
      "metadata": {
        "id": "fr7u3Mnifu03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training (MLP)"
      ],
      "metadata": {
        "id": "RGIpJ6AHHXJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, training_steps, learning_rate):\n",
        "\n",
        "  for step in range(1, training_steps + 1):\n",
        "    image_batch, label_batch = data.next_batch()\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = model.logits(image_batch)\n",
        "      xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "          logits=logits, labels=label_batch))\n",
        "\n",
        "    grads = tape.gradient(xent, model.variables)\n",
        "\n",
        "    # Perform one step of gradient descent\n",
        "    for (variable, gradient) in zip(model.variables, grads):\n",
        "      variable.assign_sub(learning_rate * gradient)\n",
        "     \n",
        "    # every so often we print loss/accuracy \n",
        "    if not step % 100:\n",
        "      preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "      acc = tf.reduce_mean(tf.cast(tf.equal(preds, label_batch),tf.float32))\n",
        "\n",
        "      print(\"Step {}. Batch loss: {} Batch accuracy: {}\".format(step+1, xent, acc))"
      ],
      "metadata": {
        "id": "iJzBcoKc-zmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "y7rqLpLpHcAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test (Original Model)"
      ],
      "metadata": {
        "id": "lCtSLLRZOhoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_preds = tf.argmax(tf.matmul(data.test_data, W) + b, axis=1,\n",
        "#                        output_type=tf.int32)\n",
        "# acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "#                              tf.float32))\n",
        "# print(acc)"
      ],
      "metadata": {
        "id": "iG4g7jq4CXm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test (Deep Model)"
      ],
      "metadata": {
        "id": "aRSpx51fOth3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = {'layers':1, 'units':100, 'activation_function':tf.nn.relu,'train_steps':2000, 'learning_rate':0.1}\n",
        "results(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il5kSoLDf-h7",
        "outputId": "d226c098-0a59-4d12-ee90-bb72bcbe78fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 101. Batch loss: 1.0678164958953857 Batch accuracy: 0.6953125\n",
            "Step 201. Batch loss: 0.6205866932868958 Batch accuracy: 0.8671875\n",
            "Step 301. Batch loss: 0.412026584148407 Batch accuracy: 0.8984375\n",
            "Step 401. Batch loss: 0.4178839921951294 Batch accuracy: 0.8984375\n",
            "Starting new epoch...\n",
            "Step 501. Batch loss: 0.36517366766929626 Batch accuracy: 0.8984375\n",
            "Step 601. Batch loss: 0.36928361654281616 Batch accuracy: 0.90625\n",
            "Step 701. Batch loss: 0.3823903203010559 Batch accuracy: 0.8828125\n",
            "Step 801. Batch loss: 0.26622289419174194 Batch accuracy: 0.9375\n",
            "Step 901. Batch loss: 0.1436774730682373 Batch accuracy: 0.9609375\n",
            "Starting new epoch...\n",
            "Step 1001. Batch loss: 0.2418626844882965 Batch accuracy: 0.9375\n",
            "Step 1101. Batch loss: 0.24137619137763977 Batch accuracy: 0.9453125\n",
            "Step 1201. Batch loss: 0.2740941047668457 Batch accuracy: 0.9140625\n",
            "Step 1301. Batch loss: 0.27896934747695923 Batch accuracy: 0.9375\n",
            "Step 1401. Batch loss: 0.2778792977333069 Batch accuracy: 0.9140625\n",
            "Starting new epoch...\n",
            "Step 1501. Batch loss: 0.206119567155838 Batch accuracy: 0.9453125\n",
            "Step 1601. Batch loss: 0.21681737899780273 Batch accuracy: 0.9140625\n",
            "Step 1701. Batch loss: 0.2021847367286682 Batch accuracy: 0.9453125\n",
            "Step 1801. Batch loss: 0.28650379180908203 Batch accuracy: 0.9140625\n",
            "Starting new epoch...\n",
            "Step 1901. Batch loss: 0.13145704567432404 Batch accuracy: 0.96875\n",
            "Step 2001. Batch loss: 0.30287063121795654 Batch accuracy: 0.8984375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus "
      ],
      "metadata": {
        "id": "ea0_owVvKXoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Fashion Dataset"
      ],
      "metadata": {
        "id": "n9v0Jll0iBnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# we can look at any of the images and the corresponding labels\n",
        "# say, image no. 155\n",
        "plt.imshow(train_images[155], cmap=\"Greys_r\")\n",
        "plt.show()\n",
        "print(train_labels[155])\n",
        "\n",
        "# images are \"flattened\" into vectors\n",
        "data = MNISTDataset(train_images.reshape([-1, 28*28]), train_labels, \n",
        "                    test_images.reshape([-1, 28*28]), test_labels,\n",
        "                    batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ljhqGXAHISdn",
        "outputId": "013c409e-6e02-48fd-ef1d-0b60d119b192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to NP Arrays"
      ],
      "metadata": {
        "id": "S9wQHqSyiG2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images.astype(np.float32) / 255\n",
        "test_images = test_images.astype(np.float32) / 255"
      ],
      "metadata": {
        "id": "CL3rb9V1iAFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Model"
      ],
      "metadata": {
        "id": "mYn84vEpiKE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = {'layers':1, 'units':100, 'activation_function':tf.nn.tanh,'train_steps':2000, 'learning_rate':0.15}\n",
        "results(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKMsXmR9QZcg",
        "outputId": "13fd718d-b7ff-4ecc-bdf6-e5ad3d4664a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 101. Batch loss: 0.7347116470336914 Batch accuracy: 0.6796875\n",
            "Step 201. Batch loss: 0.6373168230056763 Batch accuracy: 0.7578125\n",
            "Step 301. Batch loss: 0.5396754741668701 Batch accuracy: 0.8203125\n",
            "Starting new epoch...\n",
            "Step 401. Batch loss: 0.4082949161529541 Batch accuracy: 0.859375\n",
            "Step 501. Batch loss: 0.4792635440826416 Batch accuracy: 0.8359375\n",
            "Step 601. Batch loss: 0.5284992456436157 Batch accuracy: 0.78125\n",
            "Step 701. Batch loss: 0.5093953609466553 Batch accuracy: 0.8125\n",
            "Step 801. Batch loss: 0.411000519990921 Batch accuracy: 0.890625\n",
            "Starting new epoch...\n",
            "Step 901. Batch loss: 0.34168434143066406 Batch accuracy: 0.8828125\n",
            "Step 1001. Batch loss: 0.5156997442245483 Batch accuracy: 0.8203125\n",
            "Step 1101. Batch loss: 0.45732298493385315 Batch accuracy: 0.8359375\n",
            "Step 1201. Batch loss: 0.36308881640434265 Batch accuracy: 0.8671875\n",
            "Step 1301. Batch loss: 0.436737060546875 Batch accuracy: 0.859375\n",
            "Starting new epoch...\n",
            "Step 1401. Batch loss: 0.34674710035324097 Batch accuracy: 0.8671875\n",
            "Step 1501. Batch loss: 0.34798961877822876 Batch accuracy: 0.890625\n",
            "Step 1601. Batch loss: 0.31129878759384155 Batch accuracy: 0.9140625\n",
            "Step 1701. Batch loss: 0.36694246530532837 Batch accuracy: 0.875\n",
            "Starting new epoch...\n",
            "Step 1801. Batch loss: 0.38839006423950195 Batch accuracy: 0.8515625\n",
            "Step 1901. Batch loss: 0.44453126192092896 Batch accuracy: 0.8359375\n",
            "Step 2001. Batch loss: 0.42093217372894287 Batch accuracy: 0.84375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy using different activation functions\n",
        "\n",
        "\n",
        "\n",
        "1.   Sigmoid = ~82%\n",
        "2.   Tanh = ~84%\n",
        "3.   ReLu = ~85%\n",
        "\n"
      ],
      "metadata": {
        "id": "t4KkdKXAv4K2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy using different number of hidden layers\n",
        "\n",
        "1.   1 hidden layer = ~82%\n",
        "2.   2 hidden layers = ~30%\n",
        "3.   3 hidden layers = ~5%\n",
        "\n"
      ],
      "metadata": {
        "id": "h4gXhFkQwnF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy using different number of units in hidden layer\n",
        "\n",
        "1.   10 hidden units = ~71%\n",
        "2.   50 hidden units = ~79%\n",
        "3.   100 hidden units = ~82%\n",
        "\n"
      ],
      "metadata": {
        "id": "UwT-jJEIxQcl"
      }
    }
  ]
}